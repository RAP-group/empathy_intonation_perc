<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Using intonation to disambiguate meaning: The role of empathy and proficiency in L2 perceptual development</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge.css" rel="stylesheet" />
    <link href="libs/remark-css/rutgers.css" rel="stylesheet" />
    <link href="libs/remark-css/rutgers-fonts.css" rel="stylesheet" />
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Using intonation to disambiguate meaning:<br>The role of empathy and proficiency in L2 perceptual development
### <p>Joseph V. Casillas, Juan José Garrido Pozú, Nicole Rodríguez<sup>*</sup>, Kyle Parrish, Laura Fernández Arroyo, Robert Esposito, Isabelle Chang, Kimberly Gómez, Gabriela Constantin-Dureci, Jiawei Shao, Iván Andreu, and Katherine Taveras</p>
### Rutgers University | Adam Mickiewicz University<sup>*</sup>
### CASPSLaP 2022, University of Wisconsin, Madison

---








class: center
count: false

background-image: url(./libs/img/osf_dark.png), url(./libs/img/open_data.png), url(./libs/img/open_materials.png), url(./libs/img/prereg.png), url(./libs/img/PsyArXiv1.png), url(./libs/img/qr.png)
background-size: 100px, 100px, 100px, 100px, 275px, 225px
background-position: 20% 80%, 35% 80%, 50% 80%, 65% 80%, 95% 80%, 50% 50%

# &lt;br&gt;&lt;br&gt;.grey[Slides, data, code, and pre-print available]

???

This is a collaborative effort with some of my students at RU and Nicole Rodriguez at Adam Meeskeeavitch University in poland

We gave a previous iteration of this talk at HLS, but today we will present more data, with some new analyses

This project is pre-registered and currently under review

the slides, data and code are available on the OSF and a pre-print of the article is available on psyarxiv

Today Im going to talk to you about empathy. 

We all have experience with the idea of empathy, and Id bet that as humans we all probably agree that empathy is a good thing

What we hope to convince you of today in this talk is that as linguists empathy is also a good thing, and by that I mean that it is a relevant factor in the acquisition of L2 phonology. 

Before I talk more about empathy Im going to talk a bit about intonation. 

---
exclude: true

# Summary 

- The present study investigates the interplay between proficiency and individual pragmatic skills in the process of learning a new language. 
- We focus on the role of empathy in the development of second language (L2) prosody by analyzing the perception and processing of intonation in questions and statements in L2 Spanish. 
- It is common for L2 learners to struggle with L2 intonation, often resulting in comprehension and communication difficulties [@trofimovich2006learning]. 
- Previous research attests that learners gradually acquire target-language prosody as they gain proficiency in the language. 
- Concretely, the perception and processing of L2 intonation has been shown to improve in conjunction with proficiency conditional on intonation type [@bustin_2020], with polar ('yes/no') interrogatives being more difficult to process and acquire when compared with simple statements. 
- The construct empathy has been shown to influence native language processing in how listeners interpret intonation and meaning when words are ambiguous [@esteve2020empathy]. 
  - higher empathy individuals, in comparison with lower empathy individuals, appear to be more sensitive to intonation cues in the process of forming sound-meaning associations. 
- We extend this research to L2 acquisition in order to determine if individual differences in pragmatic skills affect the development of intonation in L2 processing and sentence comprehension.

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/linguistics/img/production_1.png), url(./libs/img/tune.png)
background-size: contain, contain
background-position: -60% 0%, 50% 50%

???

- We know that intonation (tune, melodic pattern of a sentence) can indicate many things, like syntactic structure, sentence type, focus, affective meaning, etc.

- And that it's realization is language-specific, dialect-specific

---
class: middle

### Learners often struggle with L2 intonation, &lt;/br&gt; resulting in comprehension and communication&lt;br&gt; difficulties (Trofimovich and Baker, 2006)

???

- We also know that L2 learners can struggle with L2 intonation, often resulting in comprehension and communication difficulties (Trofimovich and Baker, 2006)

--

background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/teaching/gifs/confused.gif)
background-size: 450px
background-position: 95% 50%

???

- It can be an indicator of L2 accent

- We have probably all had the experience where we misinterpreted somebody else's intonation in our non-native languages

- Or maybe we aren't sure if somebody is asking a question or not

---
background-image: url(./libs/img/mosby.gif)
background-size: 600px
background-color: black
class: center, middle

???

- Ironically, intonation is not generally taught in the L2 classroom 

- Unsurprisingly it is one of the last aspects of l2 phonology for learners to acquire

---

# Background

&lt;br&gt;

.big[
- L2 intonation patterns acquired gradually as proficiency increases and native-like perception seems possible
]

???

Focusing on perception of Spanish, from a series of studies we know that L2 intonation patterns seem to be mastered gradually, as proficiency increases

Moreover, there is ample evidence that highly proficient adult learners perform as well as native speakers

--

.big[
- Sentence-type matters
  - statements easier than questions
  - wh- easier than y/n
]

???

The initial difficulties that learners face seem to deal with the type of utterance they hear... or more concretely the intonation pattern associated with it

the extant literature, for ex a study by brandl et al shows that statements are generally easier than questions, and specifically, partial interrogatives (wh- questions) appear to be easier than absolute interrogatives (y/n questions)

--

.big[
- Familiarity matters
]

.footnote[(Nibert, 2005; Trimble, 2013; Brandl et. al 2020)]

???

Furthermore, familiarity with the variety also seems to be important, which makes since given that intonational contours for a given utterance type can vary from variety to variety

---
background-image: url(./libs/img/empathy.png)
background-size: contain

???

So back to *empathy*

It's one of those things that intuitively we all seem to understand, but it's actually quite difficult to define

We know that it relies on inferring the intentions, understanding the feelings, and emotions of others

Research in psychology on *empathy* has associated it with Theory of Mind and the ability to take the perspective of somebody else

A definition I like states that empathy comprises the cognitive process of identifying the emotional state of another living being as well as the affective process of experiencing a similar sensation within oneself

Importantly, recent studies have used this construct as a proxy to understanding individual pragmatic skill, particularly in research on autism

---

# Pragmatic skills

### The role of empathy

.pull-left[
.full-width[
.content-box-red[
- Individual empathic skills associated with use of intonation to infer meaning when words are ambiguous

- Better pragmatic skills (↑ empathy) = sensitive to intonation cues when forming sound–meaning associations

- Less pragmatically skills (↓ empathy) = need disambiguating material to select a referent
]
]
]

.pull-right[
&lt;img src="./libs/img/esteve.png" width="912" /&gt;
.tiny[Esteve-Gibert et al. 2020]
]

???

Focusing on pragmatic skills, recent studies by Maria D'Imperio and colleagues have looked at the construct *empathy* in conjunction with sound-meaning associations

- To give a concrete example, in a visual world paradigm study, Esteve Gibert and colleagues used eye tracking methods to show that individual empathic skills are associated with monolingual listeners' use of intonation to infer meaning when words are ambiguous

- The study was quite clever. It was set up so that participants could anticipate a referent before disambiguating lexical information was available, but they had to do this by inferring either a contrast meaning or a confirmatory meaning from the intonation contour alone

- They found that listeners with better pragmatic skills (↑ empathy) were sensitive to intonation cues when forming sound–meaning associations

- Less pragmatically skilled listeners (↓ empathy) needed the disambiguating material that came later in the time course to accurately select a referent

---
class: center, middle
background-color: black

# The big picture

???

We investigate the interplay between proficiency and individual pragmatic skills in the process of learning a new language

Specifically, we focus on the role of empathy in the development of second language (L2) prosody by analyzing the perception and processing of intonation in questions and statements in L2 Spanish (conceptual replication of Brandl, González, and Bustin (2020))

Importantly, we extend linguistic research on empathy to L2 acquisition in order to determine if individual differences in pragmatic skills affect the development of intonation in L2 processing and sentence comprehension.

---
class: title-slide-section-grey, middle

# .RUred[Research questions]

.big[
.white[
.blue[RQ1]: Is perceptual development in L2 Spanish modulated by proficiency and intonation type (i.e., Brandl et al., 2020)?  

**Hypothesis**: Accuracy will increase and processing time will decrease as a function of proficiency and intonation type. 
Yes-no questions will present the most difficulty for L2 learners of Spanish, followed by wh- questions and declarative broad focus and narrow focus statements.
]
]

---
class: title-slide-section-grey, middle

# .RUred[Research questions]

.big[
.white[
.blue[RQ2]: Do pragmatic skills---specifically, empathy---modulate the rate of development in L2 prosody?  

**Hypothesis**: Prosodic development will occur sooner and at a faster rate in higher empathy individuals. 
]
]

???

In this operationalization, 'sooner' refers to lower proficiency levels in a cross-sectional design, that is, at an earlier developmental stage than lower empathy individuals.

---
class: title-slide-section-grey, middle

# .RUred[Research questions]

.big[
.white[
.blue[RQ3]: Does speaker variety affect perception accuracy and processing speed?  

**Hypothesis**: L2 learners will have most difficulty (lower accuracy, slower RTs) with the Cuban variety. 
]
]

???

Based on tentative findings from native speaker pilot data

---













class: title-slide-section-red, middle

# Method

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/linguistics/img/perception_2.png)
background-size: 150px
background-position: 90% 20%

# Method

### Participants

.large[
- We recruited 225 participants for a two-alternative &lt;/br&gt; forced choice (2AFC) task 
- Adult L2 Spanish learners (L1 English) 
- From the Northeastern United States 
]

???

Participants were recruited online and were screened such English their L1, they started learning Spanish at the age of 13 or older?, and did not have significant knowledge of any languages other than English and Spanish

--

### Proficiency

.large[
- All participants completed LexTALE-ESP&lt;sup&gt;1&lt;/sup&gt;
- Lexical decision task, assessment of vocab size/proficiency
- x&amp;#772; &amp;equals; 
12.95 &amp;pm;
13.6 SD, range [
−16, 55]
]





.footnote[&lt;sup&gt;1&lt;/sup&gt;Izura, Cuetos, and Brysbaert (2014)]

???

Lexical Test for Advanced Learners of Spanish as a standardized assessment of 
participants' proficiency/vocabulary size in Spanish.

scores can range from &amp;minus;20 to 60, with native speaker values generally 
found between 50-60.

Individuals with little or no knowledge of Spanish typically score from 
&amp;minus;20 to 0.

proficiency is treated as a continuous variable, thus we consider a 
monolingual English speaker to have little to no proficiency in Spanish.

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/linguistics/img/perception_2.png)
background-size: 150px
background-position: 90% 20%

# Method

### Empathy

.big[
- All participants completed the Empathy Quotient (EQ) questionnaire&lt;sup&gt;1&lt;/sup&gt;
- Likert-type self-report scale
- 40 items, 20 distractors
- Max score = 80 (high empathy)
- Min score = 0 (low empathy)
- x&amp;#772; &amp;equals; 
37.88 &amp;pm;
13.39 SD, range [
9, 69]
]

.footnote[&lt;sup&gt;1&lt;/sup&gt; Baron-Cohen and Wheelwright (2004)]

---

# Method

### 2AFC

.pull-left[
.large[
- Participants presented auditory stimuli
- "Is it a question?"
- Keyboard response (1 = "yes", 0 = "no")
- 4 utterance types (n = 64)&lt;sup&gt;1&lt;/sup&gt;
  - Declarative broad focus
  - Declarative narrow focus
  - Absolute interrogative (y/n)
  - Partial Interrogative (wh-)
]
]

.pull-right[
.large[
- 8 speaker varieties
  - Madrileño (F)
  - Mexican (F)
  - Chilean (F)
  - Andalusian (F)
  - Peruvian (M)
  - Argentine (M)
  - Puerto Rican (F)
  - Cuban (F)
- Stimuli drawn randomly in 1 block
- Speaker variety also randomized (.125 prob)

]
]

.footnote[&lt;sup&gt;1&lt;/sup&gt;.grey[Brandl, González, and Bustin (2020)]]

---

# Method

### Procedure

.big[

- Single experimental session
  1. 2AFC
  2. LexTALE task
  3. Empathy quotient
]

.big[
- Experiments run in PsychoPy3 .lightgrey[(Peirce et al., 2019)] 

- Participants recruited via Prolific.ac

- Mean time to completion (all tasks): 13 minutes

]

---

# Method 

### Statistical analysis

.Large[
- Bayesian multilevel regression models fitted in `stan` using `brms` 
.lightgrey[(Bürkner, 2017)]

- Drift diffusion models

- Measurement-error models
]

???

- Responses accuracy, 0 = incorrect, 1 = correct

- Modeled as a function of *LexTALE score*, *empathy quotient*, and all higher order interactions

- Continuous predictors standardized 

- Nested group-level effects:
  - participant
  - speaker_variety/sentence_type
  - sentence_type
  - item

- 2000 iterations (1000 warm-up), 16 processing cores 

- Regularizing, weakly informative priors .lightgrey[(Gelman, Simpson, and Betancourt, 2017)]

---
background-image: url(../../../figs/manuscript/ddm_explanation.png)
background-size: contain

???

A drift diffusion model of the present study. 
The upper and lower bounds represent correct and incorrect responses 
The boundary separation (α) is the distance between the two thresholds and indicates the evidence required to make a decision. 
Non-decision time (τ) represents the time course before evidence accumulation begins, i.e., time used for any process except decision-making. 
Bias (β) is the starting point for the evidence accumulation in the vertical plane (i.e., closer or further away from a given threshold), 
and drift rate (δ) quantifies the rate of evidence accumulation. 

---
class: title-slide-section-red, middle

# Results

---
background-image: url(../../../figs/slides/learner_accuracy_by_utterance_type.png)
background-size: 95%

---
background-image: url(../../../figs/slides/learner_accuracy_lt_eq_by_utterance_type1.png)
background-size: contain

---
count: false
background-image: url(../../../figs/slides/learner_accuracy_lt_eq_by_utterance_type2.png)
background-size: contain

---
background-image: url(../../../figs/slides/learner_accuracy_3way1.png)
background-size: contain

---
count: false
background-image: url(../../../figs/slides/learner_accuracy_3way2.png)
background-size: contain

---
background-image: url(../../../figs/slides/learner_accuracy_rt_by_speaker_variety1.png)
background-size: 95%

---
count: false
background-image: url(../../../figs/slides/learner_accuracy_rt_by_speaker_variety2.png)
background-size: 95%

---
background-image: url(../../../figs/slides/mem_bs_dr_estimates.png)
background-size: 95%

---
background-image: url(../../../figs/slides/ddm_simulations0.png)
background-size: 95%

---
count: false
background-image: url(../../../figs/slides/ddm_simulations1.png)
background-size: 95%

---
count: false
background-image: url(../../../figs/slides/ddm_simulations2.png)
background-size: 95%

---
count: false
background-image: url(../../../figs/slides/ddm_simulations3.png)
background-size: 95%

---
count: false
background-image: url(../../../figs/slides/ddm_simulations4.png)
background-size: 95%

---

# Review

???

- We presented audio stimuli to adult L2 learners of Spanish
- The stimuli were questions and statements from 8 different varieties 
- The participants' task was simple. We asked: "Is this a question?" and they responded "yes" or "no"

--

.Large[
.blue[RQ1]: Is perceptual development in L2 Spanish modulated by proficiency and intonation type (i.e., Brandl et al., 2020)?
]

### Yes. 

.big[

- Proficiency positively correlated with response accuracy
- Proficiency &amp;times; utterance type interaction

]

.large[

|               |                        |                |
| :------------ | :--------------------: | -------------: |
|               | Statements → Questions |                |
| .blue[Easier] .white[........] | | .white[........] **Harder** |
| | broad focus → narrow focus → wh- → y/n | |

]

???

RQ1: Is perceptual development in L2 Spanish modulated by proficiency and intonation type (i.e., Brandl et al., 2020)?  
**Hypothesis**: Accuracy will increase and processing time will decrease as a function of proficiency and intonation type. 
Yes-no questions will present the most difficulty for L2 learners of Spanish, followed by wh- questions and declarative broad focus and narrow focus statements.

---

# Review

.big[

.blue[RQ2]: Do pragmatic skills&amp;mdash;specifically, empathy&amp;mdash;modulate the rate of development in L2 prosody?

### Probably. 

- Evidence for 3-way interaction
  - **↑** empathy quotient associated with **↑** accuracy for all utterance types except y/n questions
  - Particularly true for intermediate/advanced learners
]

--

.big[

- Cross-sectional design suggests **↑** empathy individuals may progress at earlier stage of L2 development

]

???

RQ2: Do pragmatic skills---specifically, empathy---modulate the rate of development in L2 prosody?
**Hypothesis**: Prosodic development will occur sooner and at a faster rate in higher empathy individuals. 

---

# Review

.big[

.blue[RQ3]: Does speaker variety affect perception accuracy and processing speed?  

]

### Yes! 

.big[

- **↑** accuracy for Madrileño variety (most familiar)
- **.blue[↓]** accuracy for Cuban variety (L2 and monolingual listeners)
]

--

### But...

.big[
- High accuracy does not necessarily imply faster processing
]

---

# Discussion

.pull-left[
.full-width[
.content-box-red[

### Spanish intonation

- Our findings corroborate previous studies on intonational development in Spanish

  - Development in step with proficiency measures (i.e., Nibert, 2005, 2006; Brandl et al., 2020)

  - y/n question are most difficult (Trimble, 2013; Brandl et al., 2020)

  - Familiarity with variety affects accuracy (Trimble, 2013)

]
]
]

--

.pull-right[
.full-width[
.content-box-blue[

### Empathy as pragmatic skill

- We build on previous linguistic research incorporating the construct empathy

  - Empathy is associated with intonation-meaning mapping (Esteve-Gibert, Schafer, Hemforth, Portes, Pozniak, and D’Imperio, 2020)

  - Also true in SLA/bilingualism 

  - Extend association to language proficiency and intonation type

]
]
]

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/rstats/memes/lm_correlation_not_causation.png)
background-size: 1200px

---
background-image: url(https://raw.githubusercontent.com/jvcasillas/media/master/teaching/img/think.png)
background-size: 350px
background-position: 95% 50%

# Discussion

### What about L2 phonology?

.large[
.pull-left[
- Models of L2 phonology (SLMr, PAM-L2, L2LP) don't focus on suprasegmentals

- Should account for extralinguistic constructs that correlate with development

- Growing body of evidence suggesting empathy is relevant variable

]
]

---

# Discussion

### Next steps

.big[

- Learnability of empathy

- Longitudinal perceptual development

- Empathy and variation in speech processing with regard to dialectal differences (L1, L2)

- Segments

- y/n questions?

]

---
background-image: url(https://acendahealth.org/wp-content/uploads/2021/02/empathy.jpg)
background-size: 450px
background-position: 95% 50%

# Conclusion

.large[
.pull-left[

- Perception and processing of intonation develops in tandem with proficiency in the target language and interacts with individual empathy levels

- Higher empathic individuals appear to be more sensitive to intonation cues in the process of forming sound-meaning associations

- Increased sensitivity does not necessarily entail increased processing speed

]
]

???

- We extend linguistic research on empathy and intonation-meaning mapping to L2 acquisition

- Perception and processing of intonation develops in tandem with proficiency in the target language and interacts with individual empathy levels

- this supports the general conclusion that higher empathic individuals, in comparison with lower empathic individuals, appear to be more sensitive to intonation cues in the process of forming sound-meaning associations. 

- Importantly, increased sensitivity does not necessarily entail increased processing speed.

---
exclude: true

Munro and Derwing (1995)


---
count: false
class: title-slide-final
background-color: black
background-image: url(https://github.com/jvcasillas/ru_xaringan/raw/master/img/logo/ru_shield.png), url(https://raw.githubusercontent.com/jvcasillas/hex_stickers/master/stickers/rap-group.png), url(https://edsurge.imgix.net/uploads/post/image/12460/empathy-1565029076.jpg?auto=compress%2Cformat&amp;w=1600&amp;h=648&amp;fit=crop), url(./libs/img/qr.png)
background-size: 100px, 130px, 350px, 160px
background-position: 20% 50%, 80% 50%, 50% 25%, 50% 65%

# Thank you!

&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;

.large[

|                                     |                                         |
| ----------------------------------: | :-------------------------------------- |
| <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:red;overflow:visible;position:relative;"><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg> | .lightgrey[joseph.casillas@rutgers.edu] |
| <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:red;overflow:visible;position:relative;"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>     | .lightgrey[@jvcasill]                   |
| <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:red;overflow:visible;position:relative;"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg>        | .lightgrey[bit.ly/rap-empathy]          |

]

---

# References

Baron-Cohen, S. and S. Wheelwright (2004). "The empathy quotient: an
investigation of adults with Asperger syndrome or high functioning
autism, and normal sex differences". In: _Journal of autism and
developmental disorders_ 34.2, pp. 163-175. DOI:
[10.1023/B:JADD.0000022607.19833.00](https://doi.org/10.1023%2FB%3AJADD.0000022607.19833.00).

Brandl, A., C. González, and A. Bustin (2020). "The development of
intonation in L2 Spanish: A perceptual study". In: _Hispanic
Linguistics: Current issues and new directions_. Ed. by A.
Morales-Front, M. J. Ferreira, R. P. Leow and C. Sanz. Issues in
Hispanic and Lusophone Linguistics 26. John Benjamins Publishing
Company, p. 12–31. DOI:
[10.1075/ihll.26](https://doi.org/10.1075%2Fihll.26).

Bürkner, P. (2017). "brms: An R Package for Bayesian Multilevel Models
Using Stan". In: _Journal of Statistical Software_ 80.1, pp. 1-28. DOI:
[10.18637/jss.v080.i01](https://doi.org/10.18637%2Fjss.v080.i01).

Esteve-Gibert, N., A. J. Schafer, B. Hemforth, et al. (2020). "Empathy
influences how listeners interpret intonation and meaning when words
are ambiguous". In: _Memory &amp; cognition_ 48, pp. 566-580. DOI:
[10.3758/s13421-019-00990-w](https://doi.org/10.3758%2Fs13421-019-00990-w).

Gelman, A., D. Simpson, and M. Betancourt (2017). "The Prior Can Often
Only Be Understood in the Context of the Likelihood". In: _Entropy_
19.10, pp. 1-13. ISSN: 1099-4300. DOI:
[10.3390/e19100555](https://doi.org/10.3390%2Fe19100555). URL:
[http://dx.doi.org/10.3390/e19100555](http://dx.doi.org/10.3390/e19100555).

---
count: false

# References II

Izura, C., F. Cuetos, and M. Brysbaert (2014). "LexTALE-Esp: A test to
rapidly and efficiently assess the Spanish vocabulary size". In:
_Psicológica_ 35.1, pp. 49-66. DOI:
[10.1037/t47086-000](https://doi.org/10.1037%2Ft47086-000).

Munro, M. J. and T. M. Derwing (1995). "Foreign accent,
comprehensibility, and intelligibility in the speech of second language
learners". In: _Language learning_ 45.1, pp. 73-97. DOI:
[10.1111/j.1467-1770.1995.tb00963.x](https://doi.org/10.1111%2Fj.1467-1770.1995.tb00963.x).

Peirce, J. W., J. R. Gray, S. Simpson, et al. (2019a). "PsychoPy2:
experiments in behavior made easy". In: _Behavior Research Methods_
51.1, pp. 195-203.

Trofimovich, P. and W. Baker (2006). "Learning second language
suprasegmentals: Effect of L2 experience on prosody and fluency
characteristics of L2 speech". In: _Studies in second language
acquisition_ 28.1, pp. 1-30. DOI:
[10.1017/S0272263106060013](https://doi.org/10.1017%2FS0272263106060013).

---
class: title-slide-section-grey, middle
count: false

# Extras

---
count: false
background-image: url(../../../figs/manuscript/learner_dp_utterance_variety.png)
background-size: 95%

---
count: false
background-image: url(../../../figs/manuscript/sm_speech_rate.png)
background-size: 90%

---
count: false
background-image: url(../../../figs/manuscript/sm_random_speaker_check.png)
background-size: 90%

---
count: false
class: middle

# Stimuli examples

.pull-left[
.full-width[
.content-box-blue[
Peninsular

- Declarative, broad focus

&lt;audio controls style="width: 120px;"&gt;
  &lt;source src="./libs/wavs/castilian_match_declarative-narrow-focus_Maria-bebe-el-vino.ogg" type="audio/ogg"&gt;
  &lt;source src="./libs/wavs/castilian_match_declarative-narrow-focus_Maria-bebe-el-vino.wav" type="audio/mpeg"&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

- Partial interrogative (wh- question)

&lt;audio controls style="width: 120px;"&gt;
  &lt;source src="./libs/wavs/castilian_match_interrogative-partial-wh_Por-que-ama-la-navidad.ogg" type="audio/ogg"&gt;
  &lt;source src="./libs/wavs/castilian_match_interrogative-partial-wh_Por-que-ama-la-navidad.wav" type="audio/mpeg"&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

- Absolute interrogative (y/n question)

&lt;audio controls style="width: 120px;"&gt;
  &lt;source src="./libs/wavs/castilian_match_interrogative-total-yn_Ana-lleva-el-abrigo.ogg" type="audio/ogg"&gt;
  &lt;source src="./libs/wavs/castilian_match_interrogative-total-yn_Ana-lleva-el-abrigo.wav" type="audio/mpeg"&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

]
]
]

.pull-right[
.full-width[
.content-box-red[
Cuban variety

- Declarative, broad focus

&lt;audio controls style="width: 120px;"&gt;
  &lt;source src="./libs/wavs/cuban_match_declarative-broad-focus_Maria-bebe-el-vino.ogg" type="audio/ogg"&gt;
  &lt;source src="./libs/wavs/cuban_match_declarative-broad-focus_Maria-bebe-el-vino.wav" type="audio/mpeg"&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

- Partial interrogative (wh- question)

&lt;audio controls style="width: 120px;"&gt;
  &lt;source src="./libs/wavs/cuban_match_interrogative-partial-wh_Por-que-ama-la-navidad.ogg" type="audio/ogg"&gt;
  &lt;source src="./libs/wavs/cuban_match_interrogative-partial-wh_Por-que-ama-la-navidad.wav" type="audio/mpeg"&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

- Absolute interrogative (y/n question)

&lt;audio controls style="width: 120px;"&gt;
  &lt;source src="./libs/wavs/cuban_match_interrogative-total-yn_Ana-lleva-el-abrigo.ogg" type="audio/ogg"&gt;
  &lt;source src="./libs/wavs/cuban_match_interrogative-total-yn_Ana-lleva-el-abrigo.wav" type="audio/mpeg"&gt;
Your browser does not support the audio element.
&lt;/audio&gt;

]
]
]




&lt;style type="text/css"&gt;

.title-slide {
  background-image: url(https://github.com/jvcasillas/ru_xaringan/raw/master/img/logo/ru_shield.png), url(./libs/img/poznan.jpg);
  background-position: 9% 15%, 91% 15%;
  background-size: 55px, 110px;
  background-color: #fff;
  padding-left: 100px;
}

/* H1 fonts */
.title-slide h1 {
  color: #cc0033;
  padding-top: 250px;
  font-weight: normal;
  font-size: 45px;
  text-align: left;
  text-shadow: none;
  padding-bottom: 18px;
  margin-bottom: 18px;
}

.remark-slide table {
  margin: auto;
  border: 0px none;
  border-collapse: collapse;
}
.remark-slide table thead th { 
  border: 0px none;
  background-color: none;
}

th, td { 
  padding: 5px;
  border: 0px none;
  background-color: none;
}

.remark-slide thead, .remark-slide tfoot, .remark-slide tr:nth-child(even) {
  border: 0px none;
  background: none;
}

ul.no_bullet {
  list-style-type: none;
  padding: 0;
  margin: 0;
}

li.prod {
  background: url(https://raw.githubusercontent.com/jvcasillas/media/master/linguistics/img/production_1.png) no-repeat;
  background-size: 95px 95px;
  display: block;
  padding: 0 50px 10px 150px;
}

li.perc {
  background: url(https://raw.githubusercontent.com/jvcasillas/media/master/linguistics/img/perception_1.png) no-repeat;
  background-size: 80px 95px;
  display: block;
  padding: 0 50px 10px 150px;
}

&lt;/style&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"></script>
<script src="https://www.jvcasillas.com/ru_xaringan/js/ru_xaringan.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
