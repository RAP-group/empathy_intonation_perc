## Traditional analyses

This section contains additional information regarding the response accuracy and response time analyses, as well as tables reported but not included in the main text.

**Learner response accuracy**. 

The population effects of the response accuracy model were specified in the following manner: 

$$
\begin{aligned}
\text{is_correct}_{ij} &\sim Bernoulli\left(p_{ij}, m_{ij}\right) \\
\text{logit}\left(p_{ij}\right) &= \beta_{0} + \beta_{1} * question\_type + \beta_{2} * LexTALE + \beta_{3} * EQ + \\ 
 & \ \ \ \ \beta_{4} * question\_type * LexTALE * EQ 
\end{aligned}
$$

We employed the `0 + Intercept` syntax of `brms` and set weakly informative priors as follows: 

$$
\begin{aligned}
\beta  &\sim Normal(0, 0.3) \\
\sigma &\sim Cauchy(0, 0.1) \\
\rho   &\sim LKJcorr(2)
\end{aligned}
$$

The summary of the response accuracy model is available in Table \@ref(tab:table-learner-accuracy-by-utterance-type). 
The information provided in this table is equivalent to the left panel of Figure \@ref(fig:plot-learner-accuracy-forest-by-utterance-type) in the manuscript. 

```{r, 'table-learner-accuracy-by-utterance-type'}
read_csv(here("tables", "learner_response_01.csv"), col_types = cols(.default = "c")) %>% 
  knitr::kable(format = "pandoc", align = c("l", "r", "r", "r", "r", "r"), 
    caption = "Summary of the posterior distribution modeling 
    response accuracy as a function of utterance type, LexTALE, and 
    Empathy quotient. The table includes posterior medians, the 95% HDI, 
    the percentage of the HDI within the ROPE, and the maximum probability 
    of effect (MPE).",
    label = "table-learner-accuracy-by-utterance-type")
```


```{=openxml}
<w:p><w:r><w:br w:type="page"/></w:r></w:p>
```


## Drift diffusion models

Drift Diffusion Models (DDM), also referred to as Wiener Diffusion Models and Decision Diffusion Models, represent our preferred method for analyzing the data from our 2AFC task. 
DDMs are rarely used in SLA research, though they are commonplace in psychology. 
The primary selling point of using a DDM is related to the parameters the model estimates: boundary separation (&alpha;), drift rate (&delta;), bias (&beta;), and non-decision time (&tau;). 
Together these parameters give rich information about the processes believed to underpin decision-making. 
Specifically, a DDM requires decision data, e.g., "left" or "right" choices, correct or incorrect responses, etc., and response times associated with said decisions. 
In linguistics, particularly in psycholinguistics, data of this nature derived from 2AFC tasks are often analyzed using separate models, one for responses, and another for response times (as we have done in our so-called 'traditional analyses').^[Given how relatively uncommon DDMs are in linguistics, the present work includes both approaches, though it is reasonable to assume that this practice will diminish as DDMs become more well-known and the resources for implementing them become more user-friendly.] 
As mentioned, a DDM uses both of these dependent variables---responses and response times---to estimate the 4 aforementioned parameters. 
The estimates can then be scrutinized in subsequent models, if one estimates the parameters for each participant (i.e., the approach taken in the present work), and/or used for simulations. 
For our purposes, we employ the Bayesian implementation of the DDM, thus we sample from a posterior distribution of plausible estimates for &alpha;, &delta;, &beta;, and &tau; for each participant. 
We then summarize and report these posterior distributions for statistical inferences. 

The no-pooling models were fit using the following specification in `brms`: 

```r
  rt_raw | dec(is_correct) ~ 0 + sentence_type,
  bs ~ 0 + sentence_type, 
  ndt ~ 0 + sentence_type, 
  bias ~ 0 + sentence_type
```

and the priors were: 

```r
prior("normal(0, 1)", class = "b"),
prior("normal(0, 5)", class = "b", dpar = "bs"),
prior("normal(0.2, 1)", class = "b", dpar = "ndt"),
prior("normal(0.5, 1)", class = "b", dpar = "bias")
```

The complete code used to fit the models are available in `09_ddm.R` in the r scripts directory. 

**Measurement-error models**. The measurement error models fit to the boundary separation and drift rate data were specified to include the standard error around each posterior median for &alpha; and &delta;: 

$$
\begin{split}
\alpha &\sim Normal\left(\alpha_{n,TRUE}, SE_{\alpha}\right) \\
\delta &\sim Normal(\delta_{n,TRUE}, SE_{\delta})
\end{split}
$$

The priors for the drift rate model were: 

$$
\begin{aligned}
\alpha &\sim Normal(1, 0.5) \\
\beta  &\sim Normal(0, 0.3)  \\
\tau   &\sim Cauchy(0, 0.3) \\
\sigma &\sim Cauchy(0, 0.1) \\
\rho   &\sim LKJcorr(2)
\end{aligned}
$$

and the priors for the boundary separation model were: 

$$
\begin{aligned}
\alpha &\sim Normal(2, 0.5) \\
\beta  &\sim Normal(0, 0.5)  \\
\tau   &\sim Cauchy(0, 0.3) \\
\sigma &\sim Cauchy(0, 0.1) \\
\rho   &\sim LKJcorr(2)
\end{aligned}
$$

To specify this type of model in `brms` we use the `resp_se` function, as follows: 

```r
estimate | resp_se(se, sigma = TRUE) ~ 1 + # Criterion
  q_sum * lextale_std * eq_std +           # Population-level effects
  (1 + q_sum * lextale_std * eq_std | participant) # Group-level effects
```


The model summary is available in Table \@ref(tab:table-ddm-bs-dr), which is equivalent to Figure \@ref(fig:plot-mem-bs-dr-estimates) in the main document. 

```{r, 'table-ddm-bs-dr'}
read_csv(here("tables", "ddm_bs_dr.csv"), col_types = cols(.default = "c")) %>% 
  mutate(Model = case_when(
    Model == "Boundary separation" & Parameter == "Intercept" ~ "Boundary", 
    Model == "Boundary separation" & Parameter == "Question type" ~ "separation", 
    Model == "Boundary separation" & !(Parameter %in% c("Intercept", "Question type")) ~ " ", 
    Model == "Drift rate" & Parameter == "Intercept" ~ "Drift rate", 
    Model == "Drift rate" & Parameter != "Intercept" ~ " ", 
  )) %>% 
  knitr::kable(format = "pandoc", align = c(rep("l", 2), rep("r", 5)), 
    caption = "Summary of the posterior distribution modeling 
    boundary separation and drift rate as a function of question type, 
    LexTALE, and Empathy quotient. The table includes posterior medians, 
    the 95% HDI, the percentage of the HDI within the ROPE, and the maximum 
    probability of effect (MPE).",
    label = "table-ddm-bs-dr")
```


```{=openxml}
<w:p><w:r><w:br w:type="page"/></w:r></w:p>
```


## Supplementary analyses {#supplementary-analyses}

In this section we present supplementary analyses, all of which are exploratory in nature. 

**D'**. Figure \@ref(fig:plot-learner-dp-utterance-variety) and Table \@ref(tab:table-dp-utterance-variety) represent an exploratory analysis of d' scores as a function of utterance type and speaker variety. 
One observes similar patterns to those from the accuracy analysis presented in the manuscript. 
The primary takeaway is that the analysis of learners' sensitivity to Spanish prosody mirrors that of their accuracy. 
That is to say, learners are more sensitive to (and accurate with) statements (broad focus, narrow focus) than questions (wh-, yes/no) (left panel of Figure \@ref(fig:plot-learner-dp-utterance-variety). 
Learner sensitivity to the different Spanish varieties represented in the stimuli pattern in the same manner, i.e., more sensitivity to the Peninsular variety and less sensitivity to the Cuban variety (right panel of Figure \@ref(fig:plot-learner-dp-utterance-variety)).
Table \@ref(tab:table-dp-utterance-variety) summarizes the posterior of these exploratory analyses. 

(ref:plot-learner-dp-utterance-variety) Exploratory analysis of d' as a function of utterance type and speaker variety. Points represent posterior medians &pm;66% and 95% credible intervals.

```{r, 'plot-learner-dp-utterance-variety', fig.cap="(ref:plot-learner-dp-utterance-variety)"}
knitr::include_graphics(
  here("figs", "manuscript", "learner_dp_utterance_variety.pdf")
  )
```


```{r, 'table-dp-utterance-variety'}
read_csv(here("tables", "dp_utterance_variety.csv"), col_types = cols(.default = "c")) %>% 
  mutate(Model = case_when(
    Model == "Utterance type" & Parameter == "Broad focus statement" ~ "Utterance type", 
    Model == "Utterance type" & Parameter != "Broad focus statement" ~ " ", 
    Model == "Variety" & Parameter == "Andalusian" ~ "Variety", 
    Model == "Variety" & Parameter != "Andalusian" ~ " " 
  )) %>% 
  knitr::kable(format = "pandoc", align = c(rep("l", 2), rep("r", 3)), 
    caption = "Summary of the posterior distribution modeling 
    d' as a function of question type or speaker variety. The table 
    includes posterior medians, the 95% HDI, and the maximum 
    probability of effect (MPE).",
    label = "table-dp-utterance-variety")
```



**Familiar vs. unfamiliar varieties**. At the beginning of the experimental session, participants responded to the prompt 'I am most familiar with Spanish from' and, using a drop-down window, made a selection from the following choices: I am not familiar with Spanish, Argentina, Bolivia, Chile, Colombia, Costa Rica, Cuba, Dominican Republic, Ecuador, El Salvador, Equatorial Guinea, Guatemala, Honduras, Mexico, Nicaragua, Panama, Paraguay, Peru, Philippines, Puerto Rico, Uruguay, Venezuela, Spain, and United States. 
These choices represent the countries/territories/commonwealths where Spanish is spoken as an official or co-official language with the exception of Andorra, Belize, Gibraltar, and the Philippines, which were not included by mistake. 
Table \@ref(tab:table-familiarity-summary) below summarizes the participants' responses to this question. 
Of note, participants overwhelmingly selected 'U.S. Spanish', followed by 'Mexico', 'Peninsular', and 'Not familiar'. 

```{r, 'table-familiarity-summary'}
familiarity_table %>% 
  knitr::kable(format = "pandoc", align = c("l", "r", "r"), 
    caption = "Summary of participant self-reported familiarity with Spanish.",
    label = "table-familiarity-summary")
```

We did not pre-register a hypothesis regarding participant familiarity with Spanish. 
The following analysis is exploratory in nature. 
We analyzed the data from the participants who claimed to be most familiar with a Spanish variety that was included in our speaker varieties: Peninsular and Mexican Spanish (note: we make the assumption that 'Peninsular' is most closely associated with the Madrileño speaker). 
We coded the participants' responses to familiar versus unfamiliar varieties and fit a Bayesian logistic regression model to the data. 
The model was specified similar to previous models: 

$$
\begin{aligned}
\text{is_correct}_{ij} &\sim Bernoulli\left(p_{ij}, m_{ij}\right) \\
\text{logit}\left(p_{ij}\right) &= \beta_{0} + \beta_{1} * question\_type + \beta_{2} * Familiarity \\ 
 & \ \ \ \ \beta_{3} * question\_type * Familiarity
\end{aligned}
$$

We specified grouping variables for participant, speaker variety, and individual items. 
The models estimated varying slopes for the sentence type effect for each participant and the familiarity effect for each item. 
Again, we used the `0 + Intercept` syntax of `brms` and set weakly informative priors as follows: 

$$
\begin{aligned}
\beta  &\sim Normal(0, 1) \\
\sigma &\sim Cauchy(0, 0.2) \\
\rho   &\sim LKJcorr(1)
\end{aligned}
$$

In short, we find that, marginalizing over proficiency and empathy, participants were more accurate when responding to a familiar variety. 
This is true for all utterance types to some degree, but more clearly the case for questions (likely because responses to declarative utterances were near ceiling). 
Figure \@ref(fig:plot-learner-variety-familiarity) included in the manuscript illustrates the familiarity effect.
Table \@ref(tab:table-learner-variety-familiarity-model-summary) summarizes the model output. 
For convenience, we also provide the conditional effects of response accuracy in Table \@ref(tab:table-learner-variety-familiarity-conditional-effects). 

```{r, 'table-learner-variety-familiarity-model-summary'}
read_csv(here("tables", "learner_variety_match_response_model_summary.csv")) %>% 
  knitr::kable(format = "pandoc", align = c("l", "r", "r", "r", "r", "r"), 
    caption = glue::glue("Summary of the posterior distribution modeling 
    response accuracy as a function of utterance type and 
    familiarity. The model only includes data from participants who claimed to 
    be familiar with Mexican (n =  {familiarity$Mexico$n_v}) and Peninsular 
    (n = {familiarity$Spain$n_v}) Spanish.
    The table includes posterior medians, the 95% HDI, the percentage of the 
    HDI within the ROPE, and the maximum probability of effect (MPE)."),
    label = "table-learner-variety-familiarity-model-summary")
```


```{r, 'table-learner-variety-familiarity-conditional-effects'}
read_csv(here("tables", "learner_variety_match_response_cond_effects.csv")) %>% 
  knitr::kable(format = "pandoc", align = c("l", "r", "r", "r"), 
    caption = glue::glue("Conditional effects of response accuracy as a 
    function of sentence type and familiarity with the Spanish variety. 
    Values represent posterior medians along with the 95% HDI for unfamiliar 
    and familiar conditions, along with the posterior difference 
    (familiar - unfamiliar). The posterior predictive distribution is based on 
    data from participants who claimed to be familiar with Mexican 
    (n =  {familiarity$Mexico$n_v}) and Peninsular 
    (n = {familiarity$Spain$n_v}) Spanish."), 
    label = "table-learner-variety-familiarity-conditional-effects")
```








**Rising vs. falling contours in yes/no questions**. As pointed out by one of the anonymous reviewers, in our stimuli, the yes/no questions have rising intonation for all varieties except Cuban and Puerto Rican (see *Auditory stimuli* below). 
We calculated response accuracy as a function of intonational contour (falling: Cuban, Puerto Rican; rising: other) for yes/no questions using L2 and native listener data. 
Table \@ref(tab:table-learner-native-caribbean) provides descriptive statistics. 
One can see that both listener types were least accurate when responding to the falling contours, though this is particularly true for the Cuban variety. 
This finding is illustrated in the previous section with regard to familiarity and is seen again in the following section regarding native listener data. 

```{r, 'table-learner-native-caribbean'}
learner_native_risefall_accuracy %>% 
  select(Group, Cuban, `Puerto Rican`, Other) %>% 
  knitr::kable(format = "pandoc", align = c("l", "r", "r", "r"), 
    caption = "L2 and native listener response accuracy to yes/no questions as a function of variety (Caribbean, Non-Caribbean). The stimuli from the Caribbean varieties include falling intonational contours.",
    label = "table-learner-native-caribbean")
```

Similar to what was found in our primary analyses regarding yes/no questions, in this subset of the data, empathy had no effect on response accuracy regardless of whether the stimuli was produced with falling (i.e., Caribbean: Cuban, Puerto Rican) or rising (other) intonation (Figure \@ref(fig:plot-learner-native-caribbean-accuracy)).

(ref:plot-learner-native-caribbean-accuracy) L2 and native listener response accuracy to Caribbean (Cuban, Puerto Rican) and non-Caribbean varieties for yes/no questions. Points represent by-participant means (learners) or posterior means (natives). For learners, the horizontal axis plots empathy quotient scores.

```{r, 'plot-learner-native-caribbean-accuracy', fig.cap="(ref:plot-learner-native-caribbean-accuracy)", out.width="100%"}
knitr::include_graphics(
    here("figs", "manuscript", "learner_native_caribbean_accuracy.png")
  )
```










**Monolingual response accuracy**. In preparing our materials before official data collection, we piloted the 2AFC task and the auditory stimuli on monolingual Spanish speakers. 
The purpose of collecting this pilot data was to get an assessment of task difficulty---overall and as a function of speaker variety---and to have an idea what reasonable priors would be with regard to response times. 
From this pilot, we learned that, overall, monolingual speakers were least accuracy when responding to the Cuban variety, and to some degree the Puerto Rican variety as well. 
This finding led us to hypothesize that L2 learners would also have difficulties when responding to stimuli from the same varieties.
Figure \@ref(fig:plot-learner-native-accuracy) plots the monolingual accuracy data (right panel) next to the learner accuracy data (left panel). 
The same information is also provided in numeric form in Table \@ref(tab:table-learner-native-accuracy).

(ref:plot-learner-native-accuracy) Response accuracy as a function of group (L2 learner, monolingual Spanish speaker), speaker variety (Andalusian, Argentine, Chilean, Cuban, Madrileño, Mexican, Peruvian, Puerto Rican), and utterance type (broad focus statement, narrow focus statement, wh- question, y/n question). Points represent means of the raw data surrounded by the standard error of the mean.

```{r, 'plot-learner-native-accuracy', fig.cap="(ref:plot-learner-native-accuracy)"}
knitr::include_graphics(
  here("figs", "manuscript", "learner_native_accuracy.pdf")
  )
```

```{r, 'table-learner-native-accuracy'}
read_csv(here("tables", "learner_native_accuracy.csv")) %>% 
  mutate(Type = case_when(
    Variety != "Andalusian" & Type == "Broad focus statement" ~ " ", 
    Variety != "Andalusian" & Type == "Narrow focus statement" ~ " ", 
    Variety != "Andalusian" & Type == "Wh- question" ~ " ", 
    Variety != "Andalusian" & Type == "y/n question" ~ " ", 
    TRUE ~ Type
  )) %>% 
  knitr::kable(format = "pandoc", align = c(rep("l", 2), rep("r", 2)), 
    caption = "Response accuracy as a function of group (L2 learner, 
    monolingual Spanish speaker), speaker variety (Andalusian, Argentine, 
    Chilean, Cuban, Madrileño, Mexican, Peruvian, Puerto Rican), and 
    utterance type (broad focus statement, narrow focus statement, 
    wh- question, y/n question). Each column provides the mean and standard 
    error.",
    label = "table-learner-native-accuracy")
```

As a check on the low accuracy with the Cuban and Puerto Rican varieties, we decided to explore monolingual response accuracy further. 
To this end, we looked at the monolinguals' responses when they were presented with stimuli from their own variety, e.g., an Andalusian listener responding to stimuli from the Andalusian speaker. 
In our data, this implied a subset of Andalusian, Chilean, Cuban, Madrileño, Mexican, and Puerto Rican listeners.
Figure \@ref(fig:plot-variety-matched-native-accuracy) plots the variety-matched raw accuracy scores as a function of utterance type and Table \@ref(tab:table-variety-matched-native-accuracy) provides the same information in numeric form. 
Of note, all the monolinguals in our sample were at ceiling for all utterance types when responding to speakers from their own variety.
This is taken as evidence that the auditory stimuli are accurate representations of questions and statements for these varieties.

(ref:plot-variety-matched-native-accuracy) Variety-matched accuracy of monolingual listeners as a function of utterance type (broad focus statement, narrow focus statement, wh- question, y/n question). Points represent means of the raw data surrounded by the standard error of the mean.

```{r, 'plot-variety-matched-native-accuracy', fig.cap="(ref:plot-variety-matched-native-accuracy)"}
knitr::include_graphics(
  here("figs", "manuscript", "native_variety_matched_accuracy.pdf")
  )
```

```{r, 'table-variety-matched-native-accuracy'}
read_csv(here("tables", "variety_matched_native_accuracy.csv")) %>% 
  knitr::kable(format = "pandoc", align = c("l", "r", "r", "r", "r"), 
    caption = "Variety-matched response accuracy as a function of utterance 
    type. Accuracy refers to the proportion of correct responses along with 
    the standard error of the mean.",
  label = "table-variety-matched-native-accuracy")
```



```{=openxml}
<w:p><w:r><w:br w:type="page"/></w:r></w:p>
```

## Auditory stimuli {#auditory-stimuli}

*Speakers*. The auditory stimuli consisted of eight varieties of Spanish: Cuban, Peninsular-Madrileño, Peninsular-Andalusian, Puerto Rican, Chilean, Argentine, Mexican, and Peruvian. 
The speakers from Argentina, Chile, Peru, and Spain are linguists. 
Table \@ref(tab:table-stimuli-descriptives) contains demographic information about the speakers. 

```{r, speakers}
tribble(
  ~'Country', ~'City/Variety', ~'Gender', ~'Age', 
  "Argentina", "Buenos Aires", "Male", 27, 
  "Chile", "Valparaíso", "Female", 42, 
  "Cuba", "Havana", "Female", 55, 
  "Mexico", "Mexico City", "Female", 30, 
  "Peru", "Lima", "Male", 30, 
  "Puerto Rico", "Ponce", "Female", 35, 
  "Spain", "Cádiz (Andalusia)", "Female", 35, 
  "Spain", "Madrid", "Female", 29
  ) %>% 
  knitr::kable(format = "pandoc", 
    caption = "Demographic information for the eight varieties of Spanish 
    represented in the auditory stimuli.",
    label = "table-stimuli-descriptives")
```


*Speech rate*. In order to evaluate the possibility that the speech rate of the talkers in our stimuli may have affected response accuracy, we calculated the articulation rate (syllables spoken per second during phonation time), average syllable duration (in milliseconds), and speech rate (number of syllables divided by total time) for all items (64 items &times; 8 speakers = `r 64 * 8` utterances). 
These values are provided in Table \@ref(tab:table-stimuli-sr). 

```{r}
#| label: speech-rate-table
mono_speech_rates_avg %>% 
  mutate(`Syllable duration` = round(`Avg. syllable duration` * 1000), 
    Variety = case_when(
      Variety == "Penninsular" ~ "Madrileño", 
      Variety == "Puertorican" ~ "Puerto Rican", 
      TRUE ~ .$Variety), 
    across(c("Articulation rate", "Speech rate"), specify_decimal, k = 2)) %>% 
  select(Variety, `Articulation rate`, `Syllable duration`, `Speech rate`) %>% 
  knitr::kable(format = "pandoc", align = c("l", "r", "r", "r"), 
  caption = "Average articulation rate (number of syllables divided by total 
  phonation time), syllables duration (in milliseconds), and speech rate (number 
  of syllable divided by total time) for each variety of the acoustic stimuli 
  presented to listeners.",
    label = "table-stimuli-sr")
```

Figure \@ref(fig:plot-sm-random-speech-rate) plots the posterior medians along with 66% and 95% HDI of standardized articulation rate for each variety. 
The plot shows that, generally, there was not a lot of variability between varieties. 
Though some of the slower varieties are also those in which we see higher response accuracy (compare with Figure \@ref(fig:plot-learner-accuracy-rt-by-speaker-variety)), that was not always the case. 
That is, some of the faster varieties also had high response accuracy, e.g., Mexican Spanish. 

(ref:plot-sm-random-speech-rate) Standardized articulation rate as a function of speaker variety. Points represent posterior medians along with 66% and 95% HDI. The value '0' on the horizontal axis represents the grand mean in the standardized space, which is equivalent to `r sr_grand_mean` syllables per second.

```{r, 'plot-sm-random-speech-rate', fig.cap="(ref:plot-sm-random-speech-rate)"}
knitr::include_graphics(
  here("figs", "manuscript", "sm_speech_rate.pdf")
  )
```


*Acoustic description*. In this subsection we provide examples of intonation contours for the four utterance types (broad focus state, narrow focus statement, wh- question, yes/no question) from the speakers of our eight varieties of Spanish (Cuban, Peninsular-Madrileño, Peninsular-Andalusian, Puerto Rican, Chilean, Argentine, Mexican, and Peruvian). 
Using the recordings, we generated figures that display the waveform, spectrogram, and fundamental frequency contours. 
The figures are accompanied with two annotation tiers: an orthographic tier and a tone tier. 
We use this information together to identify associations between tonal targets and specific structure within example utterances following the Autosegmental Metrical model [@pierrehumbert1980phonology]. 
We follow the conventions of the Spanish Tones and Break Indices (Sp_ToBI) and its revisions [@beckman2002intonation; @sosa2003notacion; @face2007rising; @vilaplana2008notacion]. 
Our description is not meant to be exhaustive, but rather to give the interested reader an idea of how some of the contours included in our stimuli looked. 
Here we provide a single example of each utterance type from each speaker (Figures \@ref(fig:plot-spectrogram-castilian)-\@ref(fig:plot-spectrogram-puertorican)), though we encourage the interested reader to take a look/listen to all items, which are freely available on the OSF in the `data/stimuli` directory. 

The statement utterances from our speaker of Madrid Spanish included a nuclear configuration of L\* L\%. 
The examples provided in the top row of Figure \@ref(fig:plot-spectrogram-castilian) are primarily differentiated by a prenuclear L+>H\* in the broad focus condition and L+H\* in the narrow focus condition. 
Wh- and yes/no question had a similar prenuclear distinction, respectively. 
Our speaker also tended to use a final rise (L\* ¡H\%) in wh- questions, which differs from the more common final fall. 
This is often associated with politeness [@quilis1993; @sosa2003], or possibly "[...] a nuance of interest and greater speaker involvement in the speech act" [@estebas_prieto2010, p. 35]. 
We believe we may have obtained more formal utterances, or possibly 'lab' speech due to the context in which the stimuli were recorded, i.e., in a lab.^[For more information on lab speech, see @face2003intonation and @xu2010defense.]
For more examples, see <http://prosodia.upf.edu/atlasentonacion/enquestes/espanol/madrid/index-english.html#maptask> ("¿Dónde está tu gasolina?", minute: -3:18). 
For more description of Madrileño Spanish see also @quilis1981, @quilis1987, and @sosa1999.

(ref:plot-spectrogram-castilian) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 29 year old female speaker of Madrileño Spanish.

```{r, 'plot-spectrogram-castilian', fig.cap="(ref:plot-spectrogram-castilian)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_castilian.pdf")
  )
```

Figure \@ref(fig:plot-spectrogram-andalusian) plots the four utterance types as spoken by the Andalusian speaker. 
Similar to speaker of Madrileño Spanish, we do not see a major distinction between declaratives in nuclear position (H+L\* L\%). 
We found a tendency for this speaker to a use statement-of-the-obvious pattern where we elicited narrow focus. 
We show the nuclear configuration for wh- questions as L+¡H\* L\%, but this speaker also used L+H\* L\%. 
A final rise for wh- questions is also documented in this variety [@henriksen2010development] and is associated with formality. 
For yes-no questions, the boundary tone was typlically ¡H\%. 
More information on Western Andalusian intonation is available @sosa1999 and @henriksen2012transcription. 

(ref:plot-spectrogram-andalusian) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 35 year old female speaker of Andalusian (San Fernando, Cádiz) Spanish.

```{r, 'plot-spectrogram-andalusian', fig.cap="(ref:plot-spectrogram-andalusian)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_andalusian.pdf")
  )
```

The Argentine speaker (Figure \@ref(fig:plot-spectrogram-argentine)) generally produced declaratives with L\* L\% nuclear configurations. 
Notably absent from the narrow focus stimuli is the tritonal accent, L+H\*+L [See @sosa1999; @labastia2006prosodic, among others]. 
For wh- and yes/no questions, we see L\% and HL\% nuclear boundary tones, respectively. 
For more information on Argentine intonation we recommend @colantoni2004convergence,  @colantoni2011broad, and @labastia2011procedural. 

(ref:plot-spectrogram-argentine) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 27 year old male speaker of Argentine (Buenos Aires) Spanish.

```{r, 'plot-spectrogram-argentine', fig.cap="(ref:plot-spectrogram-argentine)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_argentine.pdf")
  )
```


Figure \@ref(fig:plot-spectrogram-chilean) illustrates example utterances from the Chilean speaker. 
We found that broad and narrow focus statements differed in their nuclear configuration (H+L\* L\% vs. L+H\* L\%, respectively). 
We also noted that wh- and yes/no questions differed in the nuclear pitch accents (L\* vs. H+L\*, respectively). 
For more information on Chilean intonation, we refer the reader to @lira2000prosodia and @ortiz2003acentos. 

(ref:plot-spectrogram-chilean) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 42 year old female speaker of Chilean (Valparaíso) Spanish.

```{r, 'plot-spectrogram-chilean', fig.cap="(ref:plot-spectrogram-chilean)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_chilean.pdf")
  )
```

In Figure \@ref(fig:plot-spectrogram-cuban) we see example utterances from the Cuban speaker. 
We typically only found a distinction in broad and narrow focus items in the prenuclear pitch accents (L+H\* and L+>H\*, respectively). 
Both questions showed followed falling patterns in nuclear position. 
For example, we found L\* L\% in wh- questions and L+H\* L\% in yes/no questions. 
In prenuclear position, our speaker also used L+>H\* in yes/no questions. 
For more information, the interested reader can consult @sosa1999, or @alvord2006spanish for an account of Cuban Spanish in contact with English in Miami, Florida. 

(ref:plot-spectrogram-cuban) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 55 year old female speaker of Cuban (Havana) Spanish.

```{r, 'plot-spectrogram-cuban', fig.cap="(ref:plot-spectrogram-cuban)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_cuban.pdf")
  )
```


The Mexican speaker distinguished between broad and narrow focus using a L\* L\% pattern in the former and L+H\* H\% in the latter. 
For wh- questions in this variety, final falls are considered the unmarked configuration [@sosa2003], though we observed some rises, as in Figure \@ref(fig:plot-spectrogram-mexican) (bottom left plot). 
For yes/no questions this speaker produced a L\* ¡H\% nuclear configuration. 
Mexican Spanish is well-documented. 
We refer the reader to @quilis1981, @quilis1987, @quilis1993, @sosa1999, @butragueno2003hacia, and @butragueno2004configuraciones for more information.

(ref:plot-spectrogram-mexican) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 30 year old female speaker of Mexican (Mexico City) Spanish.

```{r, 'plot-spectrogram-mexican', fig.cap="(ref:plot-spectrogram-mexican)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_mexican.pdf")
  )
```


The statement utterances from our speaker of Peruvian Spanish included a nuclear configuration of L\* L\%. 
The examples provided in the top row of Figure \@ref(fig:plot-spectrogram-peruvian) are primarily differentiated by a prenuclear L+>H\* in the broad focus condition and L+H\* in the narrow focus condition. 
With regard to questions, this speaker often used a L+H\* L\% nuclear configuration for wh- questions, and L\* ¡H\% for yes-no questions. 
More information on Peruvian intonation is available in @sosa1999 and @orourke2005intonation. 

(ref:plot-spectrogram-peruvian) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 30 year old male speaker of Peruvian (Lima) Spanish.

```{r, 'plot-spectrogram-peruvian', fig.cap="(ref:plot-spectrogram-peruvian)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_peruvian.pdf")
  )
```

In Figure \@ref(fig:plot-spectrogram-puertorican) we see example utterances from the Puerto Rican speaker. 
We typically only found a distinction in broad and narrow focus items in the prenuclear pitch accents (L+H\* and H\*, respectively). 
On occasion this speaker produced final rises for wh- questions (L\* ¡H\%), but also often used a falling contour. 
For yes/no questions, this speaker produced ¡H\* L\% configurations. 
More information on Puerto Rican intonation is available to the reader in @quilis1981, @quilis1987, @quilis1993, @sosa1999, @sosa2003, @armstrong2010puerto, and @armstrong2012development, among many others.

(ref:plot-spectrogram-puertorican) Waveform, spectrogram, and F0 trace exemplifying a broad focus (top left), narrow focus (top right), wh- question (bottom left), and yes/no question (bottom right) from a 35 year old female speaker of Puerto Rican (Ponce) Spanish.

```{r, 'plot-spectrogram-puertorican', fig.cap="(ref:plot-spectrogram-puertorican)"}
knitr::include_graphics(
    here("figs", "stimuli", "spectrogram_puertorican.pdf")
  )
```

In sum, we see that the acoustic stimuli used for the 2AFC task of the present project presents a large amount of between-variety and within-speaker variability. 
Most notably, our declarative utterances are often difficult to distinguish and we do not often see the typical L+H\* L\%) nuclear configuration that most commonly seen in the extant literature. 
We believe this is likely due to the fact that the manner in which we elicited narrow focus differs from other studies, which tend to use more specific pragmatic contexts (narrow focus correction, narrow focus contradiction), as opposed to a narrow focus utterance produced from answering an information seeking wh- question. 
That is, it may be the case that the use of a different pragmatic context and lab speech may result in contours different from those attested in other studies. 
This likely resulted in our narrow focus condition being easier than we initially intended. 
Our method of eliciting narrow focus follows that of @bustin_2020, of which our study is a conceptual replication. 
We do not have access to their auditory stimuli, thus we cannot confirm which pitch contours were most common in their narrow focus condition. 
To illustrate the panhispanic and within-speaker variability of statements and questions, we plot the individual pitch contours from each utterance type by variety in Figure \@ref(fig:plot-stimuli-pitch-contours). 

```{=openxml}
<w:p><w:r><w:br w:type="page"/></w:r></w:p>
```

(ref:plot-stimuli-pitch-contours) Pitch contours as function of time for utterance type and variety of Spanish. Thinner, opaque lines represent individual items, and thicker, dark lines represent the average contour trajectory. F0 values were log transformed and standardized for between speaker/variety comparability.

```{r, 'plot-stimuli-pitch-contours', fig.cap="(ref:plot-stimuli-pitch-contours)"}
knitr::include_graphics(
    here("figs", "manuscript", "stimuli_pitch_contours.pdf")
  )
```




**Randomization check across participants**. For the purposes of our research questions, it was important that every participant be presented with stimuli from all of the Spanish varieties to which we had access. 
Recall that the 2AFC task contained 64 items, 16 of each utterance type. Using javascript we assigned each variety an equal probability of being selected in a given trial (0.125). 
To ensure that our randomization worked as planned (i.e., with each variety represented approximately equally across all trials and all participants), we calculated the average number of times each variety was presented in the data set (n = `r n_learners`, and `r nrow(learners)` trials). 
One can observe in Figure \@ref(fig:plot-sm-random-speaker-check) that this is indeed the case. 

(ref:plot-sm-random-speaker-check) Average number of tokens (&pm;1 SD) presented from each speaker variety across all 14,400 trials. The experiment was programmed such that each of the 8 varieties had an equal probability of being presented (`r 1/8 * 100`%) across 64 experimental trials.

```{r, 'plot-sm-random-speaker-check', fig.cap="(ref:plot-sm-random-speaker-check)"}
knitr::include_graphics(
  here("figs", "manuscript", "sm_random_speaker_check.pdf")
  )
```


*Items*. Table \@ref(tab:table-stimuli-items) provides a list of all of the target items used for each utterance type.

```{r}
#| label: print-stim

learners %>% 
  group_by(sentence_type, sentence) %>% 
  summarize(.groups = "drop") %>% 
  mutate(
    sentence = str_replace_all(sentence, "leia", "leía"), 
    sentence = str_replace_all(sentence, "comia", "comía"),
    sentence = str_replace_all(sentence, "nino", "niño"),
    sentence = str_replace_all(sentence, "rio", "río"),
    sentence = str_replace_all(sentence, "nina", "niña"),
    sentence = str_replace_all(sentence, "Maria", "María"),
    sentence = str_replace_all(sentence, "tia", "tía"),
    sentence = str_replace_all(sentence, "Cuando", "Cuándo"),
    sentence = str_replace_all(sentence, "Por que", "Por qué"),
    sentence = str_replace_all(sentence, "bebia", "bebía"),
    `Utterance type` = case_when(
      sentence_type == "declarative-broad-focus" ~ "Broad focus statement", 
      sentence_type == "declarative-narrow-focus" ~ "Narrow focus statement", 
      sentence_type == "interrogative-partial-wh" ~ "Wh- question", 
      sentence_type == "interrogative-total-yn" ~ "yes/no question"), 
    Item = sentence) %>% 
  mutate(Item = case_when(
    `Utterance type` == "Narrow focus statement" & 
      Item == "Ana lleva el abrigo" ~ "(¿Qué lleva Ana?) Ana lleva el abrigo",
    `Utterance type` == "Narrow focus statement" & 
      Item == "Daniel iba a Bolivia" ~ "(¿A dónde iba Daniel?) Daniel iba a Bolivia", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "David leía el libro" ~ "(¿Qué leía David?) David leía el libro", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "El bebe comía muy bien" ~ "(¿Cómo comía el bebé?) El bebe comía muy bien", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "El hombre mira la luna" ~ "(¿Qué mira el hombre?) El hombre mira la luna", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "El niño oye el río" ~ "(¿Qué oye el niño?) El niño oye el río", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "Emilio ama la marcha" ~ "(¿Qué ama Emilio?) Emilio ama la marcha", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "La maestra vive en Paris" ~ "(¿Dónde vive la maestra?) La maestra vive en Paris", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "La niña lava el plato" ~ "(¿Qué lava la niña?) La niña lava el plato", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "Manuela vende el carro" ~ "(¿Qué vende Manuela?) Manuela vende el carro", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "María bebe el vino" ~ "(¿Qué bebe María?) María bebe el vino", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "Maríano habla del tiempo" ~ "(¿De qué habla Mariano?) Maríano habla del tiempo", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "Marta abre el regalo" ~ "(¿Qué abre Marta?) Marta abre el regalo", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "Mi madre come la fruta" ~ "(¿Qué come tu madre?) Mi madre come la fruta", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "Mi novio viene del lago" ~ "(¿De dónde viene tu novio?) Mi novio viene del lago", 
    `Utterance type` == "Narrow focus statement" & 
      Item == "Mi tía odia la lluvia" ~ "(¿Qué odia tu tía?) Mi tía odia la lluvia", 
    TRUE ~ .$sentence)) %>% 
  mutate(
    Item = if_else(`Utterance type` %in% c("Wh- question", "yes/no question"), 
    glue("¿{.$Item}?"), .$Item)) %>% 
  arrange(`Utterance type`) %>% 
  transmute(
    `Utterance type` = case_when(
      sentence == "Ana lleva el abrigo" & sentence_type == "declarative-broad-focus" ~ .$`Utterance type`, 
      sentence == "Ana lleva el abrigo" & sentence_type == "declarative-narrow-focus" ~ .$`Utterance type`, 
      sentence == "Cuándo bebía el vino" & sentence_type == "interrogative-partial-wh" ~ .$`Utterance type`, 
      sentence == "Ana lleva el abrigo" & sentence_type == "interrogative-total-yn" ~ .$`Utterance type`, 
      TRUE ~ " "
    ), 
    Item, 
  ) %>% 
  knitr::kable(format = "pandoc", 
    caption = "Experimental items produced in auditory stimuli.",
    label = "table-stimuli-items")

```









```{=openxml}
<w:p><w:r><w:br w:type="page"/></w:r></w:p>
```

## Author contributions

(ref:plot-credit) Author contributions according to the CREDiT author roles taxonomy. Contributions are indicated as being substantial (dark diamonds) or moderate (light diamonds).

```{r, 'plot-credit', fig.cap="(ref:plot-credit)"}
knitr::include_graphics(
  here("figs", "project_contributors.pdf")
  )
```







```{=openxml}
<w:p><w:r><w:br w:type="page"/></w:r></w:p>
```

## Reproducibility information

**About this document**  

This document was written in RMarkdown using `papaja` [@R-papaja].

**Session info**  

```{r, 'session-info', comment=""}
devtools::session_info()$platform
as.data.frame(devtools::package_info())[, c(3, 8)]
```
